{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Mixture of Experts (MoE)\n",
    "\n",
    "In this notebook, we will learn about Mixture of Experts model training.\n",
    "\n",
    "## The goals\n",
    "\n",
    "The goals of this notebook are :\n",
    "* Learn how to incorporate linear experts on a simple Convolutional Network\n",
    "* Learn how to train the new Mixture of Experts CNN for classification\n",
    "\n",
    "\n",
    "### Cancel Previous Running/Pending Jobs\n",
    "\n",
    "Before moving on, check that no jobs are still running or waiting on the SLURM queue. Let's check the SLURM jobs queue by executing the following cell:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "                16  slurmpar   dli_ds    admin  R       0:17      2 slurmnode[1-2]\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the user's jobs using the `scancel` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "! scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "! squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6.1 Mixture of Experts Introduction\n",
    "\n",
    "A Mixture of Experts (MoE) is a neural network where some layers are partitioned into small groups that can be activated or not according to the context. \n",
    "This structure allows the network to learn a wider range of behaviors. The other advantage is that MoE models will require less computation as only few experts are active at a time.\n",
    "\n",
    "<img src=\"images/MOE.png\" width=\"450\" />\n",
    "\n",
    "In the recent literature, several models have been developed following the MoE structure, such as the [Switch Transformer](https://arxiv.org/pdf/2101.03961.pdf).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.2 Write the Mixture of Experts for the basline CNN\n",
    "\n",
    "Back to our CNN cifar-10  classifier model. Let's modify it to add 1 MoE layer. The convolutional layers of the CNN extract features, while the later fully connected layers are specialized for the CIFAR-10 classification problem. \n",
    "To add expert layers in the network definition, use the `deepspeed.moe.layer.MoE` as follows (modify the forward pass accordingly):\n",
    "\n",
    "```\n",
    "deepspeed.moe.layer.MoE( hidden_size=<Hidden dimension of the model>, \n",
    "                         expert=<Torch module that defines the expert>, \n",
    "                         num_experts=<Desired number of expert>, \n",
    "                         ep_size=<Desired expert-parallel world size>,\n",
    "                         ...\n",
    "                         )\n",
    "                         \n",
    "```\n",
    "\n",
    "Learn more about the DeepSpeed Mixture of Experts in the [dedicated DeepSpeed documentation.](https://deepspeed.readthedocs.io/en/latest/moe.html) \n",
    "\n",
    "Let's transform the latest fully connected layer `fc3` to a MoE layer in order to evaluate the features extracted from early layers. We will add a final classifier `fc4`.\n",
    "We already prepared the [cifar10_deepspeed_MOE.py](./code/moe/cifar10_deepspeed_MOE.py) script. Letâ€™s run it using 8 experts partitioned on 4 GPUs, which means that each GPU will handle 2 experts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-03-23 20:07:24,453] [WARNING] [runner.py:159:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2023-03-23 20:07:24,632] [INFO] [runner.py:457:main] cmd = /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 /dli/code/moe/cifar10_deepspeed_MOE.py --deepspeed --deepspeed_config /dli/code/moe/ds_config.json --moe --ep-world-size 4 --num-experts-per-layer 8 --top-k 1 --noisy-gate-policy RSample --moe-param-group --profile-execution=True --profile-name=zero0_MOE\n",
      "[2023-03-23 20:07:25,559] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "[2023-03-23 20:07:25,560] [INFO] [launch.py:96:main] 0 NCCL_DISABLE_P2P=1\n",
      "[2023-03-23 20:07:25,560] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2023-03-23 20:07:25,560] [INFO] [launch.py:109:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2023-03-23 20:07:25,560] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2023-03-23 20:07:25,560] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "[2023-03-23 20:07:25,560] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2023-03-23 20:07:26,758] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "[2023-03-23 20:07:31,249] [INFO] [logging.py:69:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 2 | expert_parallel_size: 4\n",
      "[2023-03-23 20:07:31,250] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown\n",
      "No existing process group found, creating a new group named: ep_size_4\n",
      "[2023-03-23 20:07:31,309] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert and data parallel groups with size 4\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "No existing process group found, creating a new group named: ep_size_4\n",
      "No existing process group found, creating a new group named: ep_size_4\n",
      "No existing process group found, creating a new group named: ep_size_4\n",
      "[2023-03-23 20:07:32,822] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [0]\n",
      "[2023-03-23 20:07:32,843] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [1]\n",
      "[2023-03-23 20:07:32,853] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [2]\n",
      "[2023-03-23 20:07:32,853] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [3]\n",
      "[2023-03-23 20:07:32,864] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [0, 1, 2, 3]\n",
      "[2023-03-23 20:07:33,415] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.09935641288757324 seconds\n",
      "[2023-03-23 20:07:33,962] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2023-03-23 20:07:33,962] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2023-03-23 20:07:33,963] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale\n",
      "[2023-03-23 20:07:33,967] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2023-03-23 20:07:33,967] [INFO] [engine.py:785:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
      "[2023-03-23 20:07:33,967] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f9202f55a90>\n",
      "[2023-03-23 20:07:33,967] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001], mom=[[0.8, 0.999], [0.8, 0.999]]\n",
      "[2023-03-23 20:07:33,967] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   amp_params ................... False\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   dump_state ................... False\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... {'init_scale': 32768, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
      "[2023-03-23 20:07:33,968] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   fp16_enabled ................. True\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   gradient_clipping ............ 1.0\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 32768\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   pld_params ................... False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   scheduler_name ............... WarmupLR\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 1000}\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   steps_per_print .............. 2000\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   train_batch_size ............. 16\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  4\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   world_size ................... 4\n",
      "[2023-03-23 20:07:33,969] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
      "[2023-03-23 20:07:33,970] [INFO] [config.py:1063:print]   zero_config .................. {\n",
      "    \"stage\": 0, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 5.000000e+07, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+07, \n",
      "    \"overlap_comm\": true, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": false, \n",
      "    \"offload_param\": null, \n",
      "    \"offload_optimizer\": null, \n",
      "    \"sub_group_size\": 1.000000e+09, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_16bit_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2023-03-23 20:07:33,970] [INFO] [config.py:1063:print]   zero_enabled ................. False\n",
      "[2023-03-23 20:07:33,970] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0\n",
      "[2023-03-23 20:07:33,970] [INFO] [config.py:1065:print]   json = {\n",
      "    \"train_batch_size\": 16, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.001, \n",
      "            \"betas\": [0.8, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 3e-07\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.001, \n",
      "            \"warmup_num_steps\": 1000\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"fp16_master_weights_and_grads\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 500, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1, \n",
      "        \"initial_scale_power\": 15\n",
      "    }, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 0, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+07, \n",
      "        \"reduce_bucket_size\": 5.000000e+07, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"cpu_offload\": false\n",
      "    }\n",
      "}\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10707712173461914 seconds\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.12525558471679688 seconds\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.1020815372467041 seconds\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20218181610107422 seconds\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.11856508255004883 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1020359992980957 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.202423095703125 seconds\n",
      "[W CPUAllocator.cpp:305] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "[epoch 0, iterations   100] loss: 2.305 accuracy: 9.750000 %[epoch 0, iterations   100] loss: 2.292 accuracy: 11.250000 %\n",
      "[epoch 0, iterations   100] loss: 2.306 accuracy: 6.750000 %\n",
      "\n",
      "[epoch 0, iterations   100] loss: 2.317 accuracy: 10.500000 %\n",
      "[2023-03-23 20:07:39,984] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 178\n",
      "[2023-03-23 20:07:39,984] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 178\n",
      "[2023-03-23 20:07:39,984] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 178\n",
      "[2023-03-23 20:07:39,984] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0\n",
      "[2023-03-23 20:07:39,984] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0\n",
      "[2023-03-23 20:07:39,984] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 178\n",
      "[2023-03-23 20:07:39,984] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0\n",
      "[2023-03-23 20:07:39,984] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768, reducing to 16384.0\n",
      "[2023-03-23 20:07:39,984] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0\n",
      "[2023-03-23 20:07:40,080] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 190\n",
      "[2023-03-23 20:07:40,080] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 190\n",
      "[2023-03-23 20:07:40,080] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 190\n",
      "[2023-03-23 20:07:40,081] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
      "[2023-03-23 20:07:40,081] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 190\n",
      "[2023-03-23 20:07:40,081] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
      "[2023-03-23 20:07:40,081] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
      "[2023-03-23 20:07:40,081] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
      "[2023-03-23 20:07:40,081] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0\n",
      "[epoch 0, iterations   200] loss: 2.251 accuracy: 12.875000 %\n",
      "[epoch 0, iterations   200] loss: 2.274 accuracy: 11.625000 %[epoch 0, iterations   200] loss: 2.278 accuracy: 12.500000 %\n",
      "\n",
      "[epoch 0, iterations   200] loss: 2.281 accuracy: 14.000000 %\n",
      "[epoch 0, iterations   300] loss: 2.210 accuracy: 12.666667 %[epoch 0, iterations   300] loss: 2.222 accuracy: 12.916667 %\n",
      "\n",
      "[epoch 0, iterations   300] loss: 2.233 accuracy: 12.500000 %\n",
      "[epoch 0, iterations   300] loss: 2.230 accuracy: 14.583333 %\n",
      "[2023-03-23 20:07:41,285] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 340\n",
      "[2023-03-23 20:07:41,285] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2023-03-23 20:07:41,285] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 340\n",
      "[2023-03-23 20:07:41,285] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 340\n",
      "[2023-03-23 20:07:41,285] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 340\n",
      "[2023-03-23 20:07:41,285] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2023-03-23 20:07:41,285] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2023-03-23 20:07:41,285] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2023-03-23 20:07:41,285] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0\n",
      "[epoch 0, iterations   400] loss: 2.131 accuracy: 14.187500 %\n",
      "[epoch 0, iterations   400] loss: 2.136 accuracy: 13.937500 %[epoch 0, iterations   400] loss: 2.178 accuracy: 14.000000 %\n",
      "\n",
      "[epoch 0, iterations   400] loss: 2.186 accuracy: 16.000000 %\n",
      "[epoch 0, iterations   500] loss: 2.085 accuracy: 15.350000 %\n",
      "[epoch 0, iterations   500] loss: 2.122 accuracy: 14.650000 %[epoch 0, iterations   500] loss: 2.084 accuracy: 15.250000 %\n",
      "\n",
      "[epoch 0, iterations   500] loss: 2.039 accuracy: 17.200000 %\n",
      "[epoch 0, iterations   600] loss: 2.095 accuracy: 15.583333 %\n",
      "[epoch 0, iterations   600] loss: 2.054 accuracy: 15.541667 %\n",
      "[epoch 0, iterations   600] loss: 2.062 accuracy: 16.541667 %\n",
      "[epoch 0, iterations   600] loss: 1.997 accuracy: 18.625000 %\n",
      "[epoch 0, iterations   700] loss: 2.083 accuracy: 16.535714 %[epoch 0, iterations   700] loss: 2.043 accuracy: 16.107143 %\n",
      "\n",
      "[epoch 0, iterations   700] loss: 1.988 accuracy: 17.750000 %\n",
      "[epoch 0, iterations   700] loss: 2.065 accuracy: 18.785714 %\n",
      "[epoch 0, iterations   800] loss: 1.945 accuracy: 17.906250 %\n",
      "[epoch 0, iterations   800] loss: 1.966 accuracy: 17.031250 %\n",
      "[epoch 0, iterations   800] loss: 2.024 accuracy: 18.656250 %\n",
      "[epoch 0, iterations   800] loss: 2.001 accuracy: 19.281250 %\n",
      "[2023-03-23 20:07:45,052] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 810\n",
      "[2023-03-23 20:07:45,052] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 810\n",
      "[2023-03-23 20:07:45,052] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 810\n",
      "[2023-03-23 20:07:45,052] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:07:45,052] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:07:45,052] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 810\n",
      "[2023-03-23 20:07:45,052] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:07:45,052] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:07:45,052] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[2023-03-23 20:07:45,669] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 886\n",
      "[2023-03-23 20:07:45,669] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 886\n",
      "[2023-03-23 20:07:45,669] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 886\n",
      "[2023-03-23 20:07:45,669] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2023-03-23 20:07:45,669] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2023-03-23 20:07:45,669] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2023-03-23 20:07:45,669] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 886\n",
      "[2023-03-23 20:07:45,669] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0\n",
      "[2023-03-23 20:07:45,669] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[epoch 0, iterations   900] loss: 1.982 accuracy: 18.750000 %\n",
      "[epoch 0, iterations   900] loss: 2.050 accuracy: 17.555556 %[epoch 0, iterations   900] loss: 1.980 accuracy: 19.111111 %\n",
      "\n",
      "[epoch 0, iterations   900] loss: 1.976 accuracy: 19.472222 %\n",
      "[epoch 0, iterations  1000] loss: 1.980 accuracy: 19.475000 %\n",
      "[epoch 0, iterations  1000] loss: 1.954 accuracy: 19.450000 %\n",
      "[epoch 0, iterations  1000] loss: 2.023 accuracy: 18.200000 %\n",
      "[epoch 0, iterations  1000] loss: 2.013 accuracy: 20.050000 %\n",
      "[epoch 0, iterations  1100] loss: 1.929 accuracy: 19.863636 %\n",
      "[epoch 0, iterations  1100] loss: 1.953 accuracy: 20.022727 %\n",
      "[epoch 0, iterations  1100] loss: 1.943 accuracy: 18.886364 %\n",
      "[epoch 0, iterations  1100] loss: 1.982 accuracy: 20.613636 %\n",
      "[epoch 0, iterations  1200] loss: 1.888 accuracy: 20.541667 %[epoch 0, iterations  1200] loss: 1.965 accuracy: 19.583333 %[epoch 0, iterations  1200] loss: 1.988 accuracy: 20.479167 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  1200] loss: 1.932 accuracy: 21.125000 %\n",
      "[epoch 0, iterations  1300] loss: 1.893 accuracy: 21.019231 %\n",
      "[epoch 0, iterations  1300] loss: 1.990 accuracy: 19.923077 %\n",
      "[epoch 0, iterations  1300] loss: 1.952 accuracy: 20.673077 %\n",
      "[epoch 0, iterations  1300] loss: 1.944 accuracy: 21.384615 %\n",
      "[2023-03-23 20:07:49,730] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:07:49,730] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:07:49,730] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:07:49,730] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[2023-03-23 20:07:49,731] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[2023-03-23 20:07:49,731] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[2023-03-23 20:07:49,731] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:07:49,731] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[epoch 0, iterations  1400] loss: 1.945 accuracy: 21.321429 %\n",
      "[epoch 0, iterations  1400] loss: 1.949 accuracy: 20.464286 %[epoch 0, iterations  1400] loss: 1.955 accuracy: 20.982143 %\n",
      "\n",
      "[epoch 0, iterations  1400] loss: 1.938 accuracy: 21.642857 %\n",
      "[epoch 0, iterations  1500] loss: 1.982 accuracy: 21.316667 %\n",
      "[epoch 0, iterations  1500] loss: 1.973 accuracy: 21.383333 %\n",
      "[epoch 0, iterations  1500] loss: 1.873 accuracy: 20.883333 %\n",
      "[epoch 0, iterations  1500] loss: 1.901 accuracy: 22.116667 %\n",
      "[epoch 0, iterations  1600] loss: 1.831 accuracy: 21.750000 %[epoch 0, iterations  1600] loss: 1.908 accuracy: 21.406250 %\n",
      "[epoch 0, iterations  1600] loss: 1.912 accuracy: 21.171875 %\n",
      "\n",
      "[epoch 0, iterations  1600] loss: 1.878 accuracy: 22.343750 %\n",
      "[epoch 0, iterations  1700] loss: 1.880 accuracy: 22.058824 %\n",
      "[epoch 0, iterations  1700] loss: 1.902 accuracy: 21.661765 %[epoch 0, iterations  1700] loss: 1.865 accuracy: 21.544118 %\n",
      "\n",
      "[epoch 0, iterations  1700] loss: 1.873 accuracy: 22.558824 %\n",
      "[epoch 0, iterations  1800] loss: 1.898 accuracy: 22.375000 %[epoch 0, iterations  1800] loss: 1.889 accuracy: 22.000000 %[epoch 0, iterations  1800] loss: 1.881 accuracy: 21.944444 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  1800] loss: 1.901 accuracy: 22.805556 %\n",
      "[2023-03-23 20:07:53,742] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:07:53,742] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:07:53,742] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:07:53,742] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:07:53,742] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:07:53,742] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:07:53,742] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:07:53,742] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:07:53,774] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1891\n",
      "[2023-03-23 20:07:53,774] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1891\n",
      "[2023-03-23 20:07:53,774] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1891\n",
      "[2023-03-23 20:07:53,774] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1891\n",
      "[2023-03-23 20:07:53,774] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:07:53,775] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:07:53,775] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:07:53,775] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:07:53,775] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 0, iterations  1900] loss: 1.782 accuracy: 22.815789 %\n",
      "[epoch 0, iterations  1900] loss: 1.862 accuracy: 22.263158 %[epoch 0, iterations  1900] loss: 1.877 accuracy: 22.473684 %\n",
      "\n",
      "[epoch 0, iterations  1900] loss: 1.834 accuracy: 23.328947 %\n",
      "[epoch 0, iterations  2000] loss: 1.873 accuracy: 22.787500 %\n",
      "[2023-03-23 20:07:54,637] [INFO] [logging.py:69:log_dist] [Rank 0] step=2000, skipped=6, lr=[0.001, 0.001], mom=[[0.8, 0.999], [0.8, 0.999]]\n",
      "[epoch 0, iterations  2000] loss: 1.800 accuracy: 23.300000 %\n",
      "[epoch 0, iterations  2000] loss: 1.877 accuracy: 22.775000 %\n",
      "[2023-03-23 20:07:54,638] [INFO] [timer.py:193:stop] 0/2000, SamplesPerSec=2034.1127659656456, MemAllocated=0.0GB, MaxMemAllocated=0.0GB\n",
      "[epoch 0, iterations  2000] loss: 1.927 accuracy: 23.525000 %\n",
      "[epoch 0, iterations  2100] loss: 1.840 accuracy: 23.297619 %[epoch 0, iterations  2100] loss: 1.851 accuracy: 23.559524 %\n",
      "\n",
      "[epoch 0, iterations  2100] loss: 1.798 accuracy: 23.154762 %\n",
      "[epoch 0, iterations  2100] loss: 1.888 accuracy: 23.726190 %\n",
      "[epoch 0, iterations  2200] loss: 1.847 accuracy: 23.545455 %[epoch 0, iterations  2200] loss: 1.782 accuracy: 23.875000 %\n",
      "\n",
      "[epoch 0, iterations  2200] loss: 1.858 accuracy: 23.556818 %\n",
      "[epoch 0, iterations  2200] loss: 1.823 accuracy: 24.068182 %\n",
      "[epoch 0, iterations  2300] loss: 1.813 accuracy: 24.250000 %\n",
      "[epoch 0, iterations  2300] loss: 1.822 accuracy: 23.826087 %\n",
      "[epoch 0, iterations  2300] loss: 1.741 accuracy: 24.021739 %\n",
      "[epoch 0, iterations  2300] loss: 1.821 accuracy: 24.250000 %\n",
      "[2023-03-23 20:07:57,788] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:07:57,788] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:07:57,789] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:07:57,788] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:07:57,789] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:07:57,788] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:07:57,789] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:07:57,789] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[epoch 0, iterations  2400] loss: 1.812 accuracy: 24.395833 %\n",
      "[epoch 0, iterations  2400] loss: 1.755 accuracy: 24.572917 %\n",
      "[epoch 0, iterations  2400] loss: 1.835 accuracy: 24.166667 %\n",
      "[epoch 0, iterations  2400] loss: 1.803 accuracy: 24.479167 %\n",
      "[2023-03-23 20:07:57,956] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2413\n",
      "[2023-03-23 20:07:57,956] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2413\n",
      "[2023-03-23 20:07:57,956] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2413\n",
      "[2023-03-23 20:07:57,956] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2413\n",
      "[2023-03-23 20:07:57,956] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:07:57,956] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:07:57,956] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:07:57,956] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:07:57,956] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 0, iterations  2500] loss: 1.850 accuracy: 24.900000 %\n",
      "[epoch 0, iterations  2500] loss: 1.798 accuracy: 24.470000 %\n",
      "[epoch 0, iterations  2500] loss: 1.759 accuracy: 24.910000 %\n",
      "[epoch 0, iterations  2500] loss: 1.790 accuracy: 24.790000 %\n",
      "[epoch 0, iterations  2600] loss: 1.805 accuracy: 25.144231 %\n",
      "[epoch 0, iterations  2600] loss: 1.743 accuracy: 24.875000 %\n",
      "[epoch 0, iterations  2600] loss: 1.786 accuracy: 25.211538 %\n",
      "[epoch 0, iterations  2600] loss: 1.838 accuracy: 24.980769 %\n",
      "[epoch 0, iterations  2700] loss: 1.799 accuracy: 25.416667 %\n",
      "[epoch 0, iterations  2700] loss: 1.788 accuracy: 25.361111 %\n",
      "[epoch 0, iterations  2700] loss: 1.773 accuracy: 25.120370 %\n",
      "[epoch 0, iterations  2700] loss: 1.832 accuracy: 25.166667 %\n",
      "[2023-03-23 20:08:00,580] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2741\n",
      "[2023-03-23 20:08:00,580] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2741\n",
      "[2023-03-23 20:08:00,580] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2741\n",
      "[2023-03-23 20:08:00,580] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2741\n",
      "[2023-03-23 20:08:00,581] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2023-03-23 20:08:00,581] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2023-03-23 20:08:00,581] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2023-03-23 20:08:00,581] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0\n",
      "[2023-03-23 20:08:00,581] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0\n",
      "[epoch 0, iterations  2800] loss: 1.752 accuracy: 25.642857 %\n",
      "[epoch 0, iterations  2800] loss: 1.880 accuracy: 25.392857 %\n",
      "[epoch 0, iterations  2800] loss: 1.781 accuracy: 25.732143 %\n",
      "[epoch 0, iterations  2800] loss: 1.879 accuracy: 25.330357 %\n",
      "[epoch 0, iterations  2900] loss: 1.716 accuracy: 25.939655 %\n",
      "[epoch 0, iterations  2900] loss: 1.768 accuracy: 25.637931 %\n",
      "[epoch 0, iterations  2900] loss: 1.689 accuracy: 26.086207 %\n",
      "[epoch 0, iterations  2900] loss: 1.842 accuracy: 25.551724 %\n",
      "[epoch 0, iterations  3000] loss: 1.714 accuracy: 26.458333 %\n",
      "[epoch 0, iterations  3000] loss: 1.732 accuracy: 26.300000 %\n",
      "[epoch 0, iterations  3000] loss: 1.679 accuracy: 26.100000 %\n",
      "[epoch 0, iterations  3000] loss: 1.829 accuracy: 25.758333 %\n",
      "[epoch 0, iterations  3100] loss: 1.734 accuracy: 26.604839 %[epoch 0, iterations  3100] loss: 1.819 accuracy: 26.532258 %\n",
      "\n",
      "[epoch 0, iterations  3100] loss: 1.844 accuracy: 26.379032 %\n",
      "[epoch 0, iterations  3100] loss: 1.727 accuracy: 26.201613 %\n",
      "[epoch 1, iterations   100] loss: 1.751 accuracy: 33.750000 %\n",
      "[epoch 1, iterations   100] loss: 1.700 accuracy: 34.000000 %\n",
      "[epoch 1, iterations   100] loss: 1.715 accuracy: 32.500000 %\n",
      "[epoch 1, iterations   100] loss: 1.737 accuracy: 36.500000 %\n",
      "[2023-03-23 20:08:04,871] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:04,871] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:04,871] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:04,871] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[2023-03-23 20:08:04,871] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[2023-03-23 20:08:04,871] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[2023-03-23 20:08:04,871] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:04,872] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 1024.0 to 2048.0\n",
      "[epoch 1, iterations   200] loss: 1.768 accuracy: 33.875000 %\n",
      "[epoch 1, iterations   200] loss: 1.694 accuracy: 34.125000 %\n",
      "[epoch 1, iterations   200] loss: 1.752 accuracy: 32.250000 %\n",
      "[epoch 1, iterations   200] loss: 1.796 accuracy: 33.375000 %\n",
      "[epoch 1, iterations   300] loss: 1.797 accuracy: 32.750000 %\n",
      "[epoch 1, iterations   300] loss: 1.779 accuracy: 34.166667 %\n",
      "[epoch 1, iterations   300] loss: 1.742 accuracy: 33.250000 %\n",
      "[epoch 1, iterations   300] loss: 1.649 accuracy: 34.416667 %\n",
      "[epoch 1, iterations   400] loss: 1.687 accuracy: 33.250000 %\n",
      "[epoch 1, iterations   400] loss: 1.711 accuracy: 34.562500 %\n",
      "[epoch 1, iterations   400] loss: 1.759 accuracy: 33.250000 %\n",
      "[epoch 1, iterations   400] loss: 1.707 accuracy: 34.687500 %\n",
      "[epoch 1, iterations   500] loss: 1.585 accuracy: 34.700000 %[epoch 1, iterations   500] loss: 1.768 accuracy: 34.950000 %\n",
      "\n",
      "[epoch 1, iterations   500] loss: 1.679 accuracy: 33.950000 %\n",
      "[epoch 1, iterations   500] loss: 1.665 accuracy: 35.050000 %\n",
      "[epoch 1, iterations   600] loss: 1.676 accuracy: 35.083333 %\n",
      "[epoch 1, iterations   600] loss: 1.648 accuracy: 35.791667 %\n",
      "[epoch 1, iterations   600] loss: 1.767 accuracy: 34.208333 %\n",
      "[epoch 1, iterations   600] loss: 1.703 accuracy: 35.416667 %\n",
      "[2023-03-23 20:08:08,881] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:08,881] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:08,881] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:08,881] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:08,881] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:08,881] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:08,881] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:08,881] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:09,305] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3795\n",
      "[2023-03-23 20:08:09,305] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3795\n",
      "[2023-03-23 20:08:09,305] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3795\n",
      "[2023-03-23 20:08:09,305] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3795\n",
      "[2023-03-23 20:08:09,305] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:09,305] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:09,305] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:09,305] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:09,305] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations   700] loss: 1.678 accuracy: 35.571429 %\n",
      "[epoch 1, iterations   700] loss: 1.673 accuracy: 35.928571 %\n",
      "[epoch 1, iterations   700] loss: 1.712 accuracy: 35.071429 %\n",
      "[epoch 1, iterations   700] loss: 1.743 accuracy: 35.392857 %\n",
      "[epoch 1, iterations   800] loss: 1.763 accuracy: 35.000000 %[epoch 1, iterations   800] loss: 1.556 accuracy: 36.718750 %\n",
      "\n",
      "[epoch 1, iterations   800] loss: 1.671 accuracy: 35.750000 %\n",
      "[epoch 1, iterations   800] loss: 1.696 accuracy: 35.562500 %\n",
      "[2023-03-23 20:08:10,940] [INFO] [logging.py:69:log_dist] [Rank 0] step=4000, skipped=9, lr=[0.001, 0.001], mom=[[0.8, 0.999], [0.8, 0.999]]\n",
      "[2023-03-23 20:08:10,941] [INFO] [timer.py:193:stop] 0/4000, SamplesPerSec=2039.665636806254, MemAllocated=0.0GB, MaxMemAllocated=0.0GB\n",
      "[epoch 1, iterations   900] loss: 1.726 accuracy: 35.305556 %\n",
      "[epoch 1, iterations   900] loss: 1.684 accuracy: 36.444444 %\n",
      "[epoch 1, iterations   900] loss: 1.721 accuracy: 36.055556 %\n",
      "[epoch 1, iterations   900] loss: 1.685 accuracy: 35.861111 %\n",
      "[epoch 1, iterations  1000] loss: 1.673 accuracy: 35.225000 %[epoch 1, iterations  1000] loss: 1.666 accuracy: 36.475000 %\n",
      "\n",
      "[epoch 1, iterations  1000] loss: 1.647 accuracy: 36.275000 %\n",
      "[epoch 1, iterations  1000] loss: 1.694 accuracy: 36.100000 %\n",
      "[epoch 1, iterations  1100] loss: 1.727 accuracy: 35.090909 %\n",
      "[epoch 1, iterations  1100] loss: 1.682 accuracy: 36.590909 %\n",
      "[epoch 1, iterations  1100] loss: 1.656 accuracy: 36.727273 %\n",
      "[epoch 1, iterations  1100] loss: 1.673 accuracy: 36.159091 %\n",
      "[2023-03-23 20:08:13,317] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:13,317] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:13,317] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:13,317] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:13,317] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:13,317] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:13,317] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:13,317] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[epoch 1, iterations  1200] loss: 1.723 accuracy: 35.500000 %\n",
      "[epoch 1, iterations  1200] loss: 1.632 accuracy: 36.729167 %\n",
      "[epoch 1, iterations  1200] loss: 1.686 accuracy: 37.208333 %\n",
      "[epoch 1, iterations  1200] loss: 1.640 accuracy: 36.395833 %\n",
      "[epoch 1, iterations  1300] loss: 1.684 accuracy: 35.846154 %\n",
      "[epoch 1, iterations  1300] loss: 1.514 accuracy: 37.230769 %\n",
      "[epoch 1, iterations  1300] loss: 1.771 accuracy: 37.307692 %\n",
      "[epoch 1, iterations  1300] loss: 1.631 accuracy: 36.788462 %\n",
      "[2023-03-23 20:08:14,633] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4460\n",
      "[2023-03-23 20:08:14,633] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4460\n",
      "[2023-03-23 20:08:14,633] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4460\n",
      "[2023-03-23 20:08:14,633] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4460\n",
      "[2023-03-23 20:08:14,633] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:14,633] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:14,633] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:14,633] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:14,633] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations  1400] loss: 1.647 accuracy: 36.000000 %[epoch 1, iterations  1400] loss: 1.735 accuracy: 37.125000 %[epoch 1, iterations  1400] loss: 1.638 accuracy: 37.589286 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  1400] loss: 1.702 accuracy: 36.803571 %\n",
      "[epoch 1, iterations  1500] loss: 1.710 accuracy: 36.233333 %[epoch 1, iterations  1500] loss: 1.610 accuracy: 37.200000 %\n",
      "[epoch 1, iterations  1500] loss: 1.647 accuracy: 37.516667 %\n",
      "\n",
      "[epoch 1, iterations  1500] loss: 1.703 accuracy: 36.800000 %\n",
      "[epoch 1, iterations  1600] loss: 1.587 accuracy: 36.515625 %[epoch 1, iterations  1600] loss: 1.729 accuracy: 37.031250 %\n",
      "\n",
      "[epoch 1, iterations  1600] loss: 1.645 accuracy: 37.546875 %\n",
      "[epoch 1, iterations  1600] loss: 1.549 accuracy: 37.046875 %\n",
      "[epoch 1, iterations  1700] loss: 1.667 accuracy: 36.500000 %[epoch 1, iterations  1700] loss: 1.497 accuracy: 37.338235 %\n",
      "\n",
      "[epoch 1, iterations  1700] loss: 1.618 accuracy: 37.720588 %\n",
      "[epoch 1, iterations  1700] loss: 1.650 accuracy: 37.176471 %\n",
      "[epoch 1, iterations  1800] loss: 1.650 accuracy: 36.597222 %[epoch 1, iterations  1800] loss: 1.615 accuracy: 37.555556 %[epoch 1, iterations  1800] loss: 1.519 accuracy: 38.013889 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  1800] loss: 1.571 accuracy: 37.388889 %\n",
      "[2023-03-23 20:08:18,646] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:18,646] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:18,646] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:18,646] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:18,646] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:18,646] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:18,646] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:18,646] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:18,693] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4967\n",
      "[2023-03-23 20:08:18,693] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4967\n",
      "[2023-03-23 20:08:18,693] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4967\n",
      "[2023-03-23 20:08:18,693] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4967\n",
      "[2023-03-23 20:08:18,693] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:18,693] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:18,693] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:18,693] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:18,693] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations  1900] loss: 1.587 accuracy: 37.934211 %[epoch 1, iterations  1900] loss: 1.636 accuracy: 36.723684 %\n",
      "\n",
      "[epoch 1, iterations  1900] loss: 1.702 accuracy: 37.921053 %\n",
      "[epoch 1, iterations  1900] loss: 1.605 accuracy: 37.355263 %\n",
      "[epoch 1, iterations  2000] loss: 1.553 accuracy: 36.937500 %\n",
      "[epoch 1, iterations  2000] loss: 1.513 accuracy: 38.300000 %\n",
      "[epoch 1, iterations  2000] loss: 1.580 accuracy: 37.862500 %\n",
      "[epoch 1, iterations  2000] loss: 1.710 accuracy: 37.362500 %\n",
      "[epoch 1, iterations  2100] loss: 1.557 accuracy: 38.333333 %[epoch 1, iterations  2100] loss: 1.594 accuracy: 37.988095 %\n",
      "\n",
      "[epoch 1, iterations  2100] loss: 1.644 accuracy: 37.285714 %\n",
      "[epoch 1, iterations  2100] loss: 1.713 accuracy: 37.309524 %\n",
      "[epoch 1, iterations  2200] loss: 1.609 accuracy: 37.443182 %[epoch 1, iterations  2200] loss: 1.593 accuracy: 38.613636 %\n",
      "\n",
      "[epoch 1, iterations  2200] loss: 1.646 accuracy: 38.079545 %\n",
      "[epoch 1, iterations  2200] loss: 1.495 accuracy: 37.511364 %\n",
      "[epoch 1, iterations  2300] loss: 1.585 accuracy: 37.543478 %[epoch 1, iterations  2300] loss: 1.617 accuracy: 38.673913 %\n",
      "\n",
      "[epoch 1, iterations  2300] loss: 1.620 accuracy: 38.195652 %\n",
      "[epoch 1, iterations  2300] loss: 1.589 accuracy: 37.717391 %\n",
      "[2023-03-23 20:08:22,709] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:22,709] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:22,709] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:22,709] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:22,709] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:22,709] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:22,709] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2023-03-23 20:08:22,709] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2023-03-23 20:08:22,900] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5492\n",
      "[2023-03-23 20:08:22,900] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5492\n",
      "[2023-03-23 20:08:22,900] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5492\n",
      "[2023-03-23 20:08:22,900] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5492\n",
      "[2023-03-23 20:08:22,900] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:22,900] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:22,900] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:22,900] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2023-03-23 20:08:22,900] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations  2400] loss: 1.591 accuracy: 37.739583 %\n",
      "[epoch 1, iterations  2400] loss: 1.542 accuracy: 38.833333 %\n",
      "[epoch 1, iterations  2400] loss: 1.594 accuracy: 38.375000 %\n",
      "[epoch 1, iterations  2400] loss: 1.573 accuracy: 37.822917 %\n",
      "[epoch 1, iterations  2500] loss: 1.523 accuracy: 38.240000 %\n",
      "[epoch 1, iterations  2500] loss: 1.621 accuracy: 38.900000 %\n",
      "[epoch 1, iterations  2500] loss: 1.573 accuracy: 38.440000 %\n",
      "[epoch 1, iterations  2500] loss: 1.529 accuracy: 38.090000 %\n",
      "[epoch 1, iterations  2600] loss: 1.595 accuracy: 38.278846 %\n",
      "[epoch 1, iterations  2600] loss: 1.578 accuracy: 38.971154 %\n",
      "[epoch 1, iterations  2600] loss: 1.527 accuracy: 38.730769 %\n",
      "[epoch 1, iterations  2600] loss: 1.577 accuracy: 38.173077 %\n"
     ]
    }
   ],
   "source": [
    "! deepspeed --num_gpus=4 /dli/code/moe/cifar10_deepspeed_MOE.py  \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config /dli/code/moe/ds_config.json \\\n",
    "    --moe \\\n",
    "    --ep-world-size 4 \\\n",
    "    --num-experts-per-layer 8 \\\n",
    "    --top-k 1 \\\n",
    "    --noisy-gate-policy 'RSample' \\\n",
    "    --moe-param-group \\\n",
    "    --profile-execution=True \\\n",
    "    --profile-name='zero0_MOE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/deepspeed_MOE.png\" width=\"950\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">Congratulations!</h2>\n",
    "\n",
    "The next lab will focus on deploying large neural networks.\n",
    "\n",
    "Before moving on, we need to make sure no jobs are still running or waiting in the queue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there are still jobs running or pending, execute the following cell to cancel all the admin user's jobs using the `scancel` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cancel admin user jobs\n",
    "! scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "! squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
